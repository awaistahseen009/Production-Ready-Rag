{"question": "What is the core innovation of AR-RAG in image generation?", "keywords": ["AR-RAG", "patch-level", "autoregressive", "retrieval"], "reference_answer": "AR-RAG introduces patch-level autoregressive retrieval augmentation that dynamically incorporates k-nearest neighbor retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references.", "category": "direct_fact"}
{"question": "What are the two parallel frameworks proposed to realize AR-RAG?", "keywords": ["Distribution-Augmentation in Decoding", "DAiD", "Feature-Augmentation in Decoding", "FAiD"], "reference_answer": "The two frameworks are Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy, and Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method.", "category": "direct_fact"}
{"question": "Which datasets were used to build the patch-based retrieval database in AR-RAG?", "keywords": ["CC12M", "JourneyDB", "DataComp", "retrieval database"], "reference_answer": "The patch-based retrieval database was built using images from CC12M, JourneyDB, and DataComp.", "category": "direct_fact"}
{"question": "What library is used for efficient similarity search in AR-RAG?", "keywords": ["FAISS", "similarity search", "retrieval"], "reference_answer": "The FAISS library is used to implement efficient similarity search for the retriever.", "category": "direct_fact"}
{"question": "What is the training objective for autoregressive image generation models like Janus-Pro?", "keywords": ["training objective", "autoregressive", "probability", "Janus-Pro"], "reference_answer": "The training objective is to maximize the likelihood of predicting the next discrete image token given the previous tokens and input text prompt, formally defined as argmax over the sum of probabilities P(vn|t1,t2,...,tM,v1,...,vn-1).", "category": "direct_fact"}
{"question": "How many images were sampled from CC12M for the retrieval database?", "keywords": ["5.7 million", "CC12M", "images"], "reference_answer": "5.7 million images were sampled from CC12M for the patch-based retrieval database.", "category": "numerical"}
{"question": "How many images were sampled from DataComp for the retrieval database?", "keywords": ["4.6 million", "DataComp", "images"], "reference_answer": "4.6 million images were sampled from DataComp for the retrieval database.", "category": "numerical"}
{"question": "How many image-caption pairs were used to fine-tune the backbone models?", "keywords": ["50,000", "image-caption pairs", "fine-tune"], "reference_answer": "The backbone models were fine-tuned on a dataset of 50,000 image-caption pairs sampled from CC12M and Midjourney-v6.", "category": "numerical"}
{"question": "What is the parameter count for the base Janus-Pro model?", "keywords": ["1.0B", "parameters", "Janus-Pro"], "reference_answer": "The base Janus-Pro model has 1.0 billion parameters.", "category": "numerical"}
{"question": "What FID score did Janus-Pro with FAiD achieve on Midjourney-30K?", "keywords": ["6.67", "FID", "FAiD", "Midjourney-30K"], "reference_answer": "Janus-Pro with FAiD achieved an FID score of 6.67 on the Midjourney-30K benchmark.", "category": "numerical"}
{"question": "What is the average inference time overhead for DAiD compared to base Janus-Pro?", "keywords": ["0.22%", "overhead", "DAiD", "inference"], "reference_answer": "DAiD introduces an average inference time overhead of only 0.22% compared to the base Janus-Pro model.", "category": "numerical"}
{"question": "What is the average inference time overhead for FAiD compared to base Janus-Pro?", "keywords": ["36.03%", "overhead", "FAiD", "inference"], "reference_answer": "FAiD introduces an average inference time overhead of 36.03% compared to the base Janus-Pro model.", "category": "numerical"}
{"question": "During which stage of generation does AR-RAG perform retrieval?", "keywords": ["decoding", "autoregressive", "generation step", "AR-RAG"], "reference_answer": "AR-RAG performs retrieval dynamically during the autoregressive decoding process, specifically when predicting each next image token at each generation step.", "category": "temporal"}
{"question": "When is the h-hop surrounding patch representation computed for database construction?", "keywords": ["offline", "preprocessing", "database construction"], "reference_answer": "The representation of the h-hop surrounding patches for each image patch is computed offline during the database construction phase.", "category": "temporal"}
{"question": "How does DAiD's approach differ from FAiD in incorporating retrieved information?", "keywords": ["distribution merging", "feature blending", "DAiD vs FAiD"], "reference_answer": "DAiD incorporates retrieved information by merging a sparse retrieval-based probability distribution with the model's predicted distribution. FAiD incorporates retrieved information by blending refined patch features directly into the model's hidden states using learned compatibility scores.", "category": "comparative"}
{"question": "How does AR-RAG's patch-level retrieval fundamentally differ from image-level retrieval methods like ImageRAG?", "keywords": ["granularity", "dynamic context", "patch-level", "image-level"], "reference_answer": "AR-RAG performs retrieval at the fine-grained patch level based on the evolving local context of generation, while image-level methods like ImageRAG retrieve entire images based on the global prompt. This allows AR-RAG to integrate specific visual elements selectively and avoid overcopying irrelevant structures.", "category": "comparative"}
{"question": "How does the inference time cost of DAiD compare to FAiD?", "keywords": ["low overhead", "higher overhead", "DAiD vs FAiD", "efficiency"], "reference_answer": "DAiD has a very low inference time overhead of 0.22% compared to the base model, while FAiD has a more significant but reasonable overhead of 36.03% due to its more complex feature blending operations.", "category": "comparative"}
{"question": "What problem does image-level retrieval augmentation suffer from according to qualitative analysis?", "keywords": ["overcopying", "irrelevant elements", "ImageRAG", "limitations"], "reference_answer": "Image-level retrieval augmentation suffers from overcopying irrelevant visual elements from retrieved reference images and failing to follow instructions regarding object composition, especially for prompts with multiple objects.", "category": "holistic"}
{"question": "What are the main advantages of AR-RAG's autoregressive and patch-level retrieval approach?", "keywords": ["dynamic context", "fine-grained", "compositional flexibility", "advantages"], "reference_answer": "The main advantages are: (1) Dynamic retrieval based on evolving image context rather than just the initial prompt, (2) Fine-grained integration at the patch level, (3) Greater compositional flexibility avoiding bias from global structure of retrieved images, and (4) Selective incorporation of relevant elements reducing overcopying.", "category": "holistic"}
{"question": "What is the relationship between the encoder and patch representation in the quantized autoencoder?", "keywords": ["encoder", "θenc", "patch representation", "V"], "reference_answer": "The encoder θenc takes an image I as input and produces the latent patch representation V, formally expressed as V = θenc(I).", "category": "relationship"}
{"question": "What is the relationship between retrieval weight λ and the merged distribution in DAiD?", "keywords": ["lambda", "retrieval weight", "Dmerge", "weighted average"], "reference_answer": "The retrieval weight λ controls the influence of retrieved patches in the merged distribution, computed as Dmerge = (1-λ)·Dmodel + λ·Dretrieval, where λ ∈ [0,1].", "category": "relationship"}
{"question": "Describe the step-by-step process when predicting a token in the DAiD framework.", "keywords": ["retrieval query", "top-K", "distribution", "sampling", "DAiD"], "reference_answer": "The sequence is: (1) Convert already-generated surrounding patches to representations using the codebook, (2) Use this as query to retrieve top-K patches from database, (3) Map retrieved patches to tokens and compute Dretrieval via softmax on distances, (4) Merge Dretrieval with model's Dmodel using λ, (5) Sample next token from Dmerge.", "category": "spanning"}
{"question": "Describe the full data flow for integrating a single retrieved patch in the FAiD module.", "keywords": ["retrieve", "embed", "smooth", "compatibility score", "blend", "FAiD"], "reference_answer": "For a retrieved patch: (1) It is retrieved based on context query, (2) Mapped to discrete token via codebook Z, (3) Embedded via Embimg to get hidden state, (4) Refined via multi-scale feature smoothing using current spatial hidden state, (5) Compatibility score is computed via linear projection, (6) Weighted contribution is added to hidden state of next patch.", "category": "spanning"}
{"question": "What happens during the refining retrieved patches step in FAiD and why is it necessary?", "keywords": ["multi-scale convolution", "spatial coherence", "context alignment", "FAiD"], "reference_answer": "During refining, multi-scale convolutions are applied to copies of the spatial hidden state where a retrieved patch is placed at the target position. This smooths the retrieved patch features across multiple scales based on generated surrounding context. It is necessary to ensure retrieved patches are stylistically and structurally coherent with the already generated parts of the image around the intended location.", "category": "spanning"}
{"question": "Based on results, on which type of prompts does AR-RAG show the most significant improvement?", "keywords": ["complex prompts", "multiple objects", "spatial relations", "DPG-Bench"], "reference_answer": "AR-RAG shows the most significant improvements on complex prompts featuring multiple objects, specific attributes, and spatial relationships, as evidenced by its strong performance on DPG-Bench and the TwoObj. and Position categories of GenEval.", "category": "spanning"}
{"question": "What is the main purpose of the MusT-RAG framework?", "keywords": ["MusT-RAG", "framework", "purpose", "music question answering"], "reference_answer": "The MusT-RAG framework retrieves relevant information from MusWikiDB based on similarity for music-related queries and augments the generator's prompt to generate grounded answers for text-only music question answering tasks.", "category": "direct_fact"}
{"question": "What database does the MusT-RAG retriever search for relevant information?", "keywords": ["MusWikiDB", "retriever", "database"], "reference_answer": "The MusT-RAG retriever searches MusWikiDB for relevant information.", "category": "direct_fact"}
{"question": "What are the seven categories of music knowledge used to build MusWikiDB?", "keywords": ["artists", "genres", "instruments", "history", "technology", "theory", "forms"], "reference_answer": "The seven categories are artists, genres, instruments, history, technology, theory, and forms.", "category": "direct_fact"}
{"question": "Which algorithm was used to index MusWikiDB?", "keywords": ["BM25", "indexing", "algorithm"], "reference_answer": "BM25 was used to index MusWikiDB, a classical and highly effective algorithm for ranking text relevance.", "category": "direct_fact"}
{"question": "What are the five artist section categories defined in ArtistMus?", "keywords": ["biography", "career", "discography", "artistry", "collaborations"], "reference_answer": "The five sections are biography, career, discography, artistry, and collaborations.", "category": "direct_fact"}
{"question": "How many pages does MusWikiDB contain?", "keywords": ["31K", "pages", "MusWikiDB"], "reference_answer": "MusWikiDB contains 31K pages.", "category": "numerical"}
{"question": "How many passages are in MusWikiDB?", "keywords": ["629.2K", "passages"], "reference_answer": "MusWikiDB contains 629.2K passages.", "category": "numerical"}
{"question": "What is the total token count of MusWikiDB?", "keywords": ["65.5M", "tokens", "total"], "reference_answer": "MusWikiDB contains 65.5 million tokens.", "category": "numerical"}
{"question": "How many multiple-choice questions were generated for ArtistMus after validation?", "keywords": ["1,000", "questions", "ArtistMus"], "reference_answer": "1,000 multiple-choice questions passing human validation were generated for ArtistMus.", "category": "numerical"}
{"question": "How many artists were selected for the ArtistMus benchmark?", "keywords": ["500", "artists", "selected"], "reference_answer": "500 artists were selected for ArtistMus based on topic, genre, and country.", "category": "numerical"}
{"question": "How many root genres were obtained for ArtistMus?", "keywords": ["48", "root genres"], "reference_answer": "48 root genres were obtained initially for the ArtistMus benchmark.", "category": "numerical"}
{"question": "What passage length was used after the ablation study for MusWikiDB?", "keywords": ["128 tokens", "passage size"], "reference_answer": "Passages of up to 128 tokens were used after the ablation study.", "category": "numerical"}
{"question": "How many QA pairs were used to fine-tune the Llama model?", "keywords": ["8K", "QA pairs", "fine-tune"], "reference_answer": "8K multiple-choice QA pairs generated from MusWikiDB were used for fine-tuning.", "category": "numerical"}
{"question": "In which year was the RAG technique introduced by Lewis et al.?", "keywords": ["2021", "RAG", "Lewis"], "reference_answer": "The Retrieval Augmented Generation technique was introduced in 2021 by Lewis et al.", "category": "temporal"}
{"question": "When is the h-hop surrounding patch representation computed in RAG systems?", "keywords": ["offline", "preprocessing", "indexing stage"], "reference_answer": "The representation is computed offline during the indexing stage when constructing the searchable knowledge database.", "category": "temporal"}
{"question": "How is fine-tuning compared to RAG in terms of knowledge access?", "keywords": ["closed-book exam", "open-book exam", "comparison"], "reference_answer": "Fine-tuning is compared to a closed-book exam where the model relies on internalized knowledge, while RAG is compared to an open-book exam where the model dynamically retrieves relevant information during inference.", "category": "comparative"}
{"question": "How does RAG-style fine-tuning differ from standard QA fine-tuning?", "keywords": ["context", "external passage", "input"], "reference_answer": "RAG-style fine-tuning introduces an external relevant passage as context in addition to the question, using (context, question, answer) triples, while standard QA fine-tuning relies solely on question-answer pairs without external context.", "category": "comparative"}
{"question": "How did MusWikiDB compare to Wikipedia corpus in retrieval speed?", "keywords": ["10x faster", "retrieval speed", "efficiency"], "reference_answer": "MusWikiDB achieves a 10x faster retrieval speed compared to the Wikipedia corpus while also delivering 5.9% higher performance.", "category": "comparative"}
{"question": "What gap does the ArtistMus benchmark address in music question answering?", "keywords": ["artist-related", "metadata", "benchmark gap"], "reference_answer": "ArtistMus addresses the lack of benchmarks focused on music metadata, particularly artist-related information, which is crucial in music listening contexts but inadequately represented in existing text-only MQA benchmarks.", "category": "holistic"}
{"question": "What are the main advantages of RAG over fine-tuning for domain adaptation?", "keywords": ["scalability", "up-to-date", "no retraining", "advantages"], "reference_answer": "RAG improves factual accuracy, mitigates hallucinations, provides greater transparency through source verification, is more scalable and economically efficient as it does not require updating model parameters, and can access up-to-date information without retraining.", "category": "holistic"}
{"question": "What is the relationship between passage size and contextual question performance?", "keywords": ["passage size", "contextual questions", "performance trend"], "reference_answer": "The performance on contextual questions tended to improve as the passage size decreased across all embedding models, with better results observed for smaller passage sizes like 128 tokens.", "category": "relationship"}
{"question": "What is the relationship between the retriever function and the filtered context?", "keywords": ["retriever", "top-k passages", "context"], "reference_answer": "The retriever R is defined as a function that maps a question q and database D to a filtered context c consisting of the top-k passages, where |c| = k << |D|.", "category": "relationship"}
{"question": "Describe the step-by-step process of the RAG framework.", "keywords": ["indexing", "retrieval", "generation", "workflow"], "reference_answer": "The RAG framework consists of three steps: (1) Indexing - constructing a searchable knowledge database by chunking text and creating embeddings, (2) Retrieval - scoring and selecting top-k passages based on similarity to the input question, and (3) Generation - providing retrieved context to the LLM which produces output using next-token prediction conditioned on the query, context, and previous tokens.", "category": "spanning"}
{"question": "What were the key findings regarding RAG fine-tuning versus QA fine-tuning on ArtistMus?", "keywords": ["factual performance", "contextual performance", "comparison results"], "reference_answer": "RAG fine-tuning achieved 82.4% on factual questions compared to 40.0% for QA fine-tuning (a 42.4% improvement), and 92.0% on contextual questions compared to 79.7% for QA fine-tuning (a 12.3% improvement). This demonstrates that RAG fine-tuning enhances both factual memory and contextual understanding ability.", "category": "spanning"}
{"question": "Based on the results, why does RAG perform particularly well in the music domain?", "keywords": ["rapidly evolving", "new artists", "up-to-date knowledge", "domain characteristics"], "reference_answer": "RAG is especially useful in the music domain because the domain evolves rapidly with new artists, compositions, and styles continuously emerging. RAG can access up-to-date and specialized information without retraining, making it more suitable than fine-tuning for handling the dynamic nature of music-related knowledge.", "category": "spanning"}
{"question":"What limitation of general-purpose LLMs in music-related applications does MusT-RAG aim to address?","keywords":["general-purpose LLMs","music-related applications","music-specific knowledge"],"reference_answer":"MusT-RAG addresses the limitation that general-purpose LLMs have relatively small amounts of music-specific knowledge in their training data, which reduces their effectiveness in music-related applications.","category":"direct_fact"}
{"question":"What is MusT-RAG proposed as in the context of Music Question Answering?","keywords":["MusT-RAG","Retrieval Augmented Generation","text-only MQA"],"reference_answer":"MusT-RAG is proposed as a Retrieval Augmented Generation framework designed to adapt general-purpose LLMs for text-only Music Question Answering tasks.","category":"direct_fact"}
{"question":"What music-specific resource is introduced to support retrieval in MusT-RAG?","keywords":["MusWikiDB","music-specific","vector database"],"reference_answer":"MusWikiDB is introduced as a music-specific vector database to support retrieval in the MusT-RAG framework.","category":"direct_fact"}
{"question":"What two stages of the modeling pipeline use retrieved context in MusT-RAG?","keywords":["inference","fine-tuning","context information"],"reference_answer":"MusT-RAG uses retrieved context information during both the inference stage and the fine-tuning process.","category":"direct_fact"}
{"question":"What task is defined as MQA in this paper?","keywords":["MQA","music-related questions","domain-specific knowledge"],"reference_answer":"MQA is defined as the task of providing accurate and relevant answers to music-related questions by leveraging domain-specific musical knowledge.","category":"direct_fact"}
{"question":"How many categories of music knowledge were used to construct MusWikiDB?","keywords":["seven categories","MusWikiDB","music knowledge"],"reference_answer":"Seven categories of music knowledge were used to construct MusWikiDB.","category":"numerical"}
{"question":"How many pages does MusWikiDB contain according to Table 1?","keywords":["31K","pages","MusWikiDB"],"reference_answer":"MusWikiDB contains 31K pages.","category":"numerical"}
{"question":"What is the total number of tokens in MusWikiDB?","keywords":["65.5M","total tokens","MusWikiDB"],"reference_answer":"MusWikiDB contains a total of 65.5 million tokens.","category":"numerical"}
{"question":"What passage length and overlap were finally used to split MusWikiDB text for retrieval?","keywords":["128 tokens","10% overlap","chunking"],"reference_answer":"The text was split into passages of up to 128 tokens with a 10% overlap between adjacent passages.","category":"numerical"}
{"question":"How many artists were selected to construct the ArtistMus benchmark?","keywords":["500 artists","ArtistMus","selection"],"reference_answer":"A diverse range of 500 artists was selected to construct the ArtistMus benchmark.","category":"numerical"}
{"question":"At what point does RAG retrieve external context during answer generation?","keywords":["inference time","retrieval","external context"],"reference_answer":"RAG retrieves external context during inference time when generating answers to questions.","category":"temporal"}
{"question":"When is contextual information incorporated during RAG-style fine-tuning?","keywords":["RAG-style fine-tuning","training","context"],"reference_answer":"Contextual information is incorporated during training in RAG-style fine-tuning using context, question, and answer triples.","category":"temporal"}
{"question":"How does RAG differ from fine-tuning in terms of knowledge usage at inference time?","keywords":["RAG","fine-tuning","external knowledge","inference"],"reference_answer":"RAG retrieves external knowledge dynamically at inference time, while fine-tuning relies solely on knowledge internalized during training.","category":"comparative"}
{"question":"How does MusWikiDB compare to the general Wikipedia corpus in terms of specialization and scale?","keywords":["MusWikiDB","Wikipedia corpus","music-specialized"],"reference_answer":"MusWikiDB is much smaller in scale than the Wikipedia corpus but is exclusively composed of music-specialized text, whereas Wikipedia covers general knowledge.","category":"comparative"}
{"question":"How does RAG-style fine-tuning differ from standard QA fine-tuning in input conditioning?","keywords":["RAG-style fine-tuning","QA fine-tuning","context conditioning"],"reference_answer":"RAG-style fine-tuning conditions generation on both the question and retrieved context, while standard QA fine-tuning conditions only on the question.","category":"comparative"}
{"question":"What shortcomings of existing MQA benchmarks motivated the creation of ArtistMus?","keywords":["existing benchmarks","metadata","artist-centric questions"],"reference_answer":"Existing MQA benchmarks inadequately represent rich music metadata such as artist discographies, collaborations, and career details, motivating the creation of ArtistMus.","category":"holistic"}
{"question":"What advantages does MusT-RAG demonstrate over traditional fine-tuning approaches?","keywords":["MusT-RAG","traditional fine-tuning","factual","contextual"],"reference_answer":"MusT-RAG demonstrates improved factual accuracy and contextual understanding compared to traditional fine-tuning, while remaining scalable and adaptable.","category":"holistic"}
{"question":"What factors contribute to MusWikiDB being more computationally efficient than Wikipedia-based retrieval?","keywords":["MusWikiDB","computational efficiency","retrieval speed"],"reference_answer":"MusWikiDB’s smaller, music-focused corpus enables faster retrieval and higher relevance compared to large general-purpose Wikipedia corpora.","category":"holistic"}
{"question":"What is the formal relationship between the retriever input and output in the RAG framework?","keywords":["retriever","R(q,D)","top-k passages"],"reference_answer":"The retriever is defined as R:(q,D)→c, where q is the question, D is the database, and c is the set of top-k retrieved passages.","category":"relationship"}
{"question":"What is the relationship between passage embeddings and similarity scoring in retrieval?","keywords":["embedding","cosine similarity","retrieval"],"reference_answer":"Each passage and question is embedded into a shared vector space, and cosine similarity between embeddings is used to score and rank passages.","category":"relationship"}
{"question":"Describe the sequence of steps in the RAG framework from indexing to generation.","keywords":["indexing","retrieval","generation","RAG pipeline"],"reference_answer":"The process involves indexing the corpus with embeddings, retrieving top-k relevant passages based on similarity to the question, and conditioning the generator on the question, retrieved context, and previous tokens to generate the answer.","category":"spanning"}
{"question":"How is RAG-style fine-tuning formulated differently from standard QA fine-tuning in terms of loss?","keywords":["LRAGFine-tuning","LQAFine-tuning","loss function"],"reference_answer":"Standard QA fine-tuning minimizes loss conditioned on the question only, while RAG-style fine-tuning minimizes loss conditioned on both the question and retrieved context.","category":"spanning"}
{"question":"What steps were used to construct and validate questions in the ArtistMus benchmark?","keywords":["question generation","GPT-4o","validation","ArtistMus"],"reference_answer":"Questions were generated using GPT-4o from MusWikiDB sections, validated for music relevance and faithfulness, and then filtered through human validation.","category":"spanning"}
{"question":"How do the in-domain and out-of-domain evaluations demonstrate the robustness of MusT-RAG?","keywords":["in-domain","out-of-domain","ArtistMus","TrustMus"],"reference_answer":"MusT-RAG shows strong performance improvements on both ArtistMus and TrustMus, indicating robustness across seen and unseen music domains.","category":"spanning"}
{"question":"What overall conclusion is drawn about MusT-RAG’s effectiveness for text-only MQA?","keywords":["MusT-RAG","text-only MQA","effectiveness"],"reference_answer":"The paper concludes that MusT-RAG effectively adapts general-purpose LLMs to text-only MQA by improving factual accuracy, contextual understanding, and generalization.","category":"holistic"}
{"question":"What core problem in Retrieval-Augmented Generation (RAG) does Blended RAG aim to solve?","keywords":["RAG accuracy","retriever scalability","irrelevant context"],"reference_answer":"Blended RAG aims to address the degradation of RAG accuracy as document corpora scale, where retrievers increasingly struggle to extract the most relevant context for the generator.","category":"direct_fact"}
{"question":"What is Blended RAG proposed as in this paper?","keywords":["Blended RAG","semantic search","hybrid retrievers"],"reference_answer":"Blended RAG is proposed as a method that blends semantic search techniques, including dense vector and sparse encoder indices, with hybrid query strategies to improve retriever and RAG accuracy.","category":"direct_fact"}
{"question":"Which two main components fundamentally determine the effectiveness of a RAG system?","keywords":["Retriever","Generator","RAG components"],"reference_answer":"The effectiveness of a RAG system fundamentally depends on the Retriever (R) and the Generator (G), where the Generator is typically an LLM.","category":"direct_fact"}
{"question":"What role does the retriever play in a RAG pipeline according to the authors?","keywords":["retriever role","context retrieval","RAG"],"reference_answer":"The retriever searches large document corpora to identify relevant information and provide grounded context to the generator for accurate answer generation.","category":"direct_fact"}
{"question":"Which traditional keyword-based retrieval algorithm is discussed as a baseline in this paper?","keywords":["BM25","keyword-based retrieval","baseline"],"reference_answer":"The BM25 algorithm is discussed as a traditional keyword-based retrieval baseline.","category":"direct_fact"}
{"question":"Which three types of indices are evaluated to construct blended retrievers?","keywords":["BM25","Dense Vector","Sparse Encoder"],"reference_answer":"The three evaluated indices are BM25 for keyword-based search, Dense Vector (KNN) indices, and Sparse Encoder-based semantic indices.","category":"direct_fact"}
{"question":"What similarity measure is commonly used in dense vector retrieval methods discussed in the paper?","keywords":["cosine similarity","dense vectors","KNN"],"reference_answer":"Cosine similarity is commonly used to measure vector proximity in dense vector retrieval methods.","category":"direct_fact"}
{"question":"What limitation of current RAG systems motivated the authors to focus on retrievers rather than generators?","keywords":["retriever limitation","irrelevant context","LLM tuning"],"reference_answer":"The authors observed that tuning or prompting LLMs has limited impact when retrievers supply irrelevant context, making retriever accuracy a critical bottleneck.","category":"holistic"}
{"question":"What datasets are used to benchmark retriever performance in this study?","keywords":["NQ","TREC-COVID","HotPotQA"],"reference_answer":"Retriever performance is benchmarked using the Natural Questions (NQ), TREC-COVID, and HotPotQA datasets.","category":"direct_fact"}
{"question":"What retrieval accuracy metric is primarily used to select the best blended retrievers?","keywords":["Top-10 retrieval accuracy","retriever evaluation"],"reference_answer":"Top-10 retrieval accuracy is primarily used to identify the best blended retrievers.","category":"direct_fact"}
{"question":"Which blended retriever achieved the highest Top-10 retrieval accuracy on the NQ dataset?","keywords":["Sparse Encoder","Best Fields","NQ"],"reference_answer":"The hybrid query using a Sparse Encoder with Best Fields achieved the highest Top-10 retrieval accuracy on the NQ dataset at 88.77%.","category":"numerical"}
{"question":"What Top-10 retrieval accuracy was achieved on the TREC-COVID dataset with relevance score 2 using blended retrievers?","keywords":["TREC-COVID","Top-10 accuracy","98%"],"reference_answer":"The Sparse Encoder with Best Fields hybrid query achieved a Top-10 retrieval accuracy of 98% on the TREC-COVID dataset with relevance score 2.","category":"numerical"}
{"question":"How did blended retrievers perform on the HotPotQA dataset despite computational constraints?","keywords":["HotPotQA","hybrid queries","Sparse Encoder"],"reference_answer":"Despite constraints, blended retrievers using Sparse Encoder with Best Fields achieved the highest performance at 65.70% Top-10 retrieval accuracy.","category":"numerical"}
{"question":"What does NDCG@10 measure in the context of retriever benchmarking?","keywords":["NDCG@10","ranking quality","information retrieval"],"reference_answer":"NDCG@10 measures ranking quality by evaluating the relevance of retrieved documents while discounting lower-ranked results.","category":"direct_fact"}
{"question":"How much did Blended RAG improve NDCG@10 over the monoT5-3B benchmark on the NQ dataset?","keywords":["NDCG@10","NQ dataset","5.8% improvement"],"reference_answer":"Blended RAG improved the NDCG@10 score by 5.8% over the monoT5-3B benchmark on the NQ dataset.","category":"numerical"}
{"question":"What NDCG@10 score did Blended RAG achieve on the TREC-COVID dataset?","keywords":["NDCG@10","TREC-COVID","0.87"],"reference_answer":"Blended RAG achieved an NDCG@10 score of 0.87 on the TREC-COVID dataset.","category":"numerical"}
{"question":"Which LLM was used to ensure fair comparison during RAG experimentation?","keywords":["FLAN-T5-XXL","LLM","controlled experiments"],"reference_answer":"FLAN-T5-XXL was used for all RAG experiments to avoid the effects of LLM size or type.","category":"direct_fact"}
{"question":"How does Blended RAG differ from RAG-original and RAG-end2end on the SQuAD dataset?","keywords":["zero-shot","no fine-tuning","Blended RAG"],"reference_answer":"Blended RAG does not use SQuAD-specific fine-tuning, relying instead on optimized blended retrievers, unlike RAG-original and RAG-end2end.","category":"comparative"}
{"question":"What F1 score did Blended RAG achieve on the SQuAD dataset?","keywords":["SQuAD","F1 score","68.4"],"reference_answer":"Blended RAG achieved an F1 score of 68.4 on the SQuAD dataset.","category":"numerical"}
{"question":"By how much did Blended RAG improve Exact Match (EM) on the NQ dataset compared to prior benchmarks?","keywords":["Exact Match","NQ dataset","35% improvement"],"reference_answer":"Blended RAG improved Exact Match on the NQ dataset by 35% compared to prior benchmarks.","category":"numerical"}
{"question":"What trade-off exists between dense and sparse vector indices according to the discussion?","keywords":["dense vs sparse","storage","query speed"],"reference_answer":"Dense indices offer faster indexing but slower querying and higher storage costs, while sparse indices have slower indexing but faster querying and lower storage requirements.","category":"comparative"}
{"question":"Why are sparse encoder indices recommended for very large corpora like HotPotQA?","keywords":["HotPotQA","sparse encoder","efficiency"],"reference_answer":"Sparse encoder indices are recommended because they require significantly less storage and enable faster querying compared to dense vector indices.","category":"holistic"}
{"question":"How does the absence of metadata affect blended retriever performance?","keywords":["metadata","CoQA","hybrid queries"],"reference_answer":"Without metadata, as in the CoQA dataset, hybrid queries offer little improvement over basic queries, limiting blended retriever effectiveness.","category":"holistic"}
{"question":"What limitation of current evaluation metrics for RAG systems do the authors highlight?","keywords":["NDCG","F1","human alignment"],"reference_answer":"The authors note that NDCG, F1, and EM scores are poor proxies for human-aligned evaluation of generative Q&A systems.","category":"holistic"}
{"question":"What overarching conclusion is drawn about Blended RAG’s effectiveness?","keywords":["Blended RAG","zero-shot","generalization"],"reference_answer":"The study concludes that Blended RAG significantly improves retriever and RAG accuracy in a zero-shot setting, outperforming fine-tuned systems and enabling robust, scalable generative Q&A.","category":"holistic"}
{"question":"What is the main focus of computer vision systems reviewed for road vehicles?","keywords":["computer vision systems","road vehicles","traffic safety","driver assistance"],"reference_answer":"The main focus is on computer vision systems built into road vehicles to assist the driver, increase traffic safety, improve road capacity, and enhance travel comfort from a traffic engineering perspective.","category":"direct_fact"}
{"question":"What are the primary safety problems of road vehicles identified in the review?","keywords":["safety problems","frontal crashes","lane departure","surrounding vehicles","road signalization"],"reference_answer":"The primary safety problems include frontal crashes, lane departure collisions, accidents involving surrounding vehicles, and failures to detect or recognize road signalization.","category":"direct_fact"}
{"question":"Which two broad categories of sensors are used in vehicle computer vision systems?","keywords":["active sensors","passive sensors","vehicle vision systems"],"reference_answer":"Sensors are divided into active sensors, such as radar and laser sensors, and passive sensors, such as visible and infrared cameras.","category":"direct_fact"}
{"question":"What is the main goal of lane departure warning systems?","keywords":["lane departure warning","driver awareness","lane markers"],"reference_answer":"The main goal is to make the driver aware when the vehicle is leaving the current lane and to focus the driver’s attention on a potentially dangerous action.","category":"direct_fact"}
{"question":"Which image processing transformation is used to estimate distance between vehicles?","keywords":["perspective transformation","distance estimation","projection"],"reference_answer":"Perspective transformation is used to estimate the distance between the camera and objects in space by assigning a third coordinate to image pixels.","category":"direct_fact"}
{"question":"How many fatalities worldwide are related to road traffic accidents per year according to the statistics cited?","keywords":["12 million","fatalities","road traffic accidents"],"reference_answer":"There are about 12 million fatalities related to road traffic accidents per year worldwide.","category":"numerical"}
{"question":"How many injuries worldwide are related to road traffic accidents per year?","keywords":["50 million","injuries","road traffic accidents"],"reference_answer":"Approximately 50 million injuries related to road traffic accidents occur per year worldwide.","category":"numerical"}
{"question":"How many people were killed in traffic accidents in Croatia in 2011?","keywords":["418 killed","Croatia","2011"],"reference_answer":"In 2011, 418 people were killed in traffic accidents in Croatia.","category":"numerical"}
{"question":"What percentage of severe traffic accidents are caused by driver fatigue according to the survey study?","keywords":["30%","driver fatigue","severe accidents"],"reference_answer":"Driver fatigue is the main cause of about 30% of all severe traffic accidents.","category":"numerical"}
{"question":"What image resolution is used by the low-resolution camera in driver fatigue detection systems?","keywords":["320x240","low-resolution camera","driver fatigue"],"reference_answer":"The low-resolution camera typically uses an image resolution of 320x240 pixels.","category":"numerical"}
{"question":"When does a computer vision system need to deliver processed data in vehicle applications?","keywords":["real time","critical situation","vehicle speed"],"reference_answer":"Processed data must be available in real time, immediately after a possible critical situation is detected, with timing dependent on vehicle speed.","category":"temporal"}
{"question":"At what stage are haptic or audio warnings triggered in frontal crash prevention systems?","keywords":["frontal crash","detection","driver warning"],"reference_answer":"Haptic or audio warnings are triggered after a dangerous frontal crash situation is detected and before the crash occurs.","category":"temporal"}
{"question":"When does the system start producing false detections in driver fatigue detection based on head direction?","keywords":["30 degrees","false detections","head direction"],"reference_answer":"False detections start to appear when the driver’s head is turned 30 degrees or more away from the camera.","category":"temporal"}
{"question":"How do active sensors differ from passive sensors in vehicle detection tasks?","keywords":["active sensors","passive sensors","distance measurement"],"reference_answer":"Active sensors directly measure distance using time-of-flight principles, while passive sensors rely on captured images and visual information without emitting signals.","category":"comparative"}
{"question":"How does the Hough transformation compare to Haar-like-features methods in terms of accuracy and resource usage?","keywords":["Hough transformation","Haar-like-features","accuracy","resources"],"reference_answer":"Hough transformation is more precise but requires more system resources, while Haar-like-features methods use fewer resources but have lower accuracy.","category":"comparative"}
{"question":"How do stereo-vision methods compare to motion-based methods for vehicle detection?","keywords":["stereo vision","motion-based methods","vehicle detection"],"reference_answer":"Stereo-vision methods rely on disparity or perspective mapping to estimate depth, while motion-based methods use optical flow to separate moving objects from static background.","category":"comparative"}
{"question":"What challenges do image-level vehicle detection systems face in outdoor environments?","keywords":["environmental conditions","rain","fog","snow","robustness"],"reference_answer":"Challenges include variability in lighting, weather conditions such as rain, fog and snow, poor road signalization, and the need for robustness under changing outdoor conditions.","category":"holistic"}
{"question":"What are the main advantages of using computer vision systems for traffic safety in vehicles?","keywords":["traffic safety","driver assistance","accident reduction"],"reference_answer":"Advantages include improved detection of dangerous situations, increased driver awareness, support for collision avoidance, lane keeping, and reduction of accidents caused by human error.","category":"holistic"}
{"question":"What open problems remain in vehicle computer vision systems according to the discussion section?","keywords":["open problems","hardware limitations","real-time processing","robustness"],"reference_answer":"Open problems include hardware limitations, high computational demands, robustness to environmental conditions, and reliable real-time processing of high-level features.","category":"holistic"}
{"question":"What is the relationship between environmental conditions and system accuracy in vehicle computer vision?","keywords":["environmental conditions","system accuracy","weather"],"reference_answer":"Environmental conditions such as fog, rain, snow, and poor illumination directly reduce system accuracy and require more robust algorithms and sensors.","category":"relationship"}
{"question":"What is the relationship between vehicle speed and computation time requirements in vision systems?","keywords":["vehicle speed","computation time","real time"],"reference_answer":"As vehicle speed increases, the available computation time shortens, requiring faster and more efficient real-time processing.","category":"relationship"}
{"question":"Describe the processing pipeline for lane detection systems.","keywords":["image enhancement","edge detection","lane feature extraction","road shape estimation"],"reference_answer":"The pipeline includes image enhancement and edge detection, followed by lane feature extraction and estimation of road shape from the processed image.","category":"spanning"}
{"question":"Describe the two main phases of traffic sign detection and recognition.","keywords":["color segmentation","shape-based segmentation","classification"],"reference_answer":"The first phase performs color and shape-based segmentation to localize traffic signs, and the second phase classifies the signs using methods such as SVM, template matching, or neural networks.","category":"spanning"}
{"question":"Explain the full process of motion-based vehicle detection using optical flow.","keywords":["optical flow","candidate detection","verification","speed threshold"],"reference_answer":"The process divides the image into sub-images, computes optical flow to detect movement, identifies vehicle candidates, and verifies them using speed thresholds and appearance-based methods.","category":"spanning"}
{"question":"Based on reviewed systems, which vehicle applications most commonly use computer vision today?","keywords":["lane detection","traffic sign detection","parking assistant","adaptive cruise control"],"reference_answer":"Common applications include lane detection and departure warning, traffic sign detection, parking assistants, adaptive cruise control, obstacle detection, and driver fatigue detection.","category":"spanning"}
{"question":"What problem does D-YOLO aim to address in object detection?","keywords":["D-YOLO","adverse weather","fog","object detection"],"reference_answer":"D-YOLO aims to address the decline in object detection performance caused by adverse weather conditions such as fog, rain, and haze, which degrade image quality and introduce domain shift between training and testing data.","category":"direct_fact"}
{"question":"What are the three main components of the D-YOLO architecture?","keywords":["Clear feature extraction subnetwork","Feature adaption subnetwork","Detection subnetwork"],"reference_answer":"The three main components are the Clear Feature Extraction subnetwork, the Feature Adaption subnetwork, and the Detection subnetwork.","category":"direct_fact"}
{"question":"Which backbone network is used for the Clear Feature Extraction subnetwork?","keywords":["DarkNet53","Clear feature extraction","backbone"],"reference_answer":"DarkNet53 is used as the backbone network for the Clear Feature Extraction subnetwork.","category":"direct_fact"}
{"question":"Which module is responsible for transferring haze-free information to the detection network?","keywords":["Feature adaption module","haze-free","transfer"],"reference_answer":"The Feature Adaption module is responsible for transferring haze-free information from the Clear Feature Extraction subnetwork to the detection network.","category":"direct_fact"}
{"question":"What attention mechanism is used inside the Feature Adaption module?","keywords":["CBAM","Feature adaption module","attention"],"reference_answer":"The Feature Adaption module uses the Convolutional Block Attention Module (CBAM).","category":"direct_fact"}
{"question":"How many foggy images are generated for training in the VOC-Foggy dataset?","keywords":["9578","VOC-Foggy","training images"],"reference_answer":"9578 foggy images are generated for training in the VOC-Foggy dataset.","category":"numerical"}
{"question":"What value is used for the global atmospheric light parameter A when generating synthetic fog?","keywords":["A","0.5","atmospheric scattering"],"reference_answer":"The global atmospheric light parameter A is set to 0.5.","category":"numerical"}
{"question":"What is the range of the atmospheric scattering parameter β used to control fog density?","keywords":["beta","0.07","0.12","fog level"],"reference_answer":"The atmospheric scattering parameter β is randomly set between 0.07 and 0.12.","category":"numerical"}
{"question":"How many epochs are used to train D-YOLO in total?","keywords":["100 epochs","training"],"reference_answer":"D-YOLO is trained for a total of 100 epochs.","category":"numerical"}
{"question":"What batch size is used during training?","keywords":["batch size","16"],"reference_answer":"The batch size used during training is 16.","category":"numerical"}
{"question":"When is the Clear Feature Extraction subnetwork active during the D-YOLO pipeline?","keywords":["training phase","CFE","inference"],"reference_answer":"The Clear Feature Extraction subnetwork is active only during the training phase and is disabled during inference.","category":"temporal"}
{"question":"At which stage are the weights of the Feature Adaption subnetwork frozen during training?","keywords":["30 epochs","70 epochs","Feature adaption","frozen"],"reference_answer":"The Feature Adaption subnetwork is frozen after the first 30 epochs, during the remaining 70 epochs of training.","category":"temporal"}
{"question":"How does D-YOLO differ from sequential dehaze-and-detect methods in handling foggy images?","keywords":["feature-level fusion","sequential","dehaze and detect"],"reference_answer":"D-YOLO replaces sequential dehazing and detection with feature-level adaptation and fusion, avoiding explicit image restoration and reducing loss of latent information.","category":"comparative"}
{"question":"How does the attention feature fusion module differ from channel-only attention methods?","keywords":["r×r pooling","spatial attention","channel attention"],"reference_answer":"The attention feature fusion module uses r×r average pooling instead of global average pooling, allowing it to model both spatial and channel dependencies simultaneously.","category":"comparative"}
{"question":"How does ODConv differ from standard convolution in the Feature Adaption module?","keywords":["ODConv","dynamic convolution","multi-dimensional attention"],"reference_answer":"ODConv applies multi-dimensional attention across spatial, channel, filter, and kernel dimensions, whereas standard convolution uses fixed kernels without dynamic weighting.","category":"comparative"}
{"question":"What limitation of image-level dehazing methods motivates feature-level adaptation in D-YOLO?","keywords":["latent information","image restoration","generalization"],"reference_answer":"Image-level dehazing methods may remove important latent features and generalize poorly to real-world fog, motivating feature-level adaptation instead.","category":"holistic"}
{"question":"Why is combining hazy and dehazed features beneficial for detection under adverse weather?","keywords":["feature fusion","hazy features","dehazed features"],"reference_answer":"Combining hazy and dehazed features preserves important spatial information from hazy inputs while leveraging cleaner representations, improving robustness in adverse conditions.","category":"holistic"}
{"question":"What role does domain shift play in object detection under foggy conditions?","keywords":["domain shift","clear images","foggy images"],"reference_answer":"Domain shift between clear training images and foggy testing images leads to degraded detection performance, which D-YOLO mitigates through feature adaptation.","category":"holistic"}
{"question":"What is the relationship between Fc and Fd in the Feature Adaption module?","keywords":["Fc","Fd","KL divergence"],"reference_answer":"Fc represents clear features from the Clear Feature Extraction subnetwork, while Fd represents adapted dehazed features, with their channel distributions aligned using KL divergence loss.","category":"relationship"}
{"question":"What is the relationship between hazy features and dehazed features in the attention feature fusion module?","keywords":["Fh","Fd","fusion"],"reference_answer":"Hazy features and dehazed features are fused through attention-calibrated convolutions to produce a combined representation that balances both sources.","category":"relationship"}
{"question":"Describe the sequence of operations used to compute the attention map in the attention feature fusion module.","keywords":["point-wise addition","average pooling","convolution","sigmoid"],"reference_answer":"Hazy and dehazed features are added, pooled with r×r average pooling, passed through convolution and upsampling, combined with a shortcut, and normalized with a sigmoid to produce the attention map.","category":"spanning"}
{"question":"Explain how synthetic fog images are generated for training using the atmospheric scattering model.","keywords":["I(x)","J(x)","t(x)","A","beta"],"reference_answer":"Synthetic fog is generated using I(x)=J(x)t(x)+A(1−t(x)), where t(x)=e−βd(x), with A set to 0.5 and β sampled between 0.07 and 0.12 based on scene depth.","category":"spanning"}
{"question":"Describe the step-by-step feature flow from hazy input to final detection in D-YOLO.","keywords":["hazy input","CFE","FA","AFM","detection head"],"reference_answer":"Hazy input is processed by the detection backbone, clear images pass through CFE during training, features are adapted via FA, fused with hazy features using AFM, and sent to the detection head for bounding box prediction.","category":"spanning"}
{"question":"On which datasets does D-YOLO show consistent performance improvement over baseline YOLOv8?","keywords":["RTTS","FoggyCityscapes","Foggy Driving","RainyCityscapes"],"reference_answer":"D-YOLO shows consistent performance improvement on RTTS, FoggyCityscapes, Foggy Driving, and RainyCityscapes datasets.","category":"spanning"}
{"question":"What is the main goal of the YOLO-CL algorithm?","keywords":["YOLO-CL","galaxy cluster detection","SDSS","deep convolutional network"],"reference_answer":"The main goal of YOLO-CL is to detect galaxy clusters in the Sloan Digital Sky Survey using a modified version of the YOLO deep convolutional object detection network optimized for cluster detection.","category":"direct_fact"}
{"question":"Which existing deep learning architecture is YOLO-CL based on?","keywords":["YOLO","YOLOv3","object detection","deep learning"],"reference_answer":"YOLO-CL is based on the YOLO object detection architecture, specifically the YOLOv3 implementation.","category":"direct_fact"}
{"question":"Which cluster catalog was used to train and validate YOLO-CL?","keywords":["redMaPPer","SDSS DR8","training","validation"],"reference_answer":"YOLO-CL was trained and validated using the redMaPPer DR8 cluster catalog from the Sloan Digital Sky Survey.","category":"direct_fact"}
{"question":"What type of input data was used to train YOLO-CL?","keywords":["color images","g-band","r-band","i-band","SDSS"],"reference_answer":"YOLO-CL was trained on color images constructed from SDSS g, r, and i-band imaging data.","category":"direct_fact"}
{"question":"What detection task does YOLO-CL perform instead of galaxy photometry-based methods?","keywords":["direct detection","no photometric redshifts","no galaxy catalogs"],"reference_answer":"YOLO-CL performs direct detection of galaxy clusters from images without requiring galaxy photometric catalogs or photometric redshift measurements.","category":"direct_fact"}
{"question":"How many redMaPPer clusters were used in the final training and validation sample?","keywords":["24,406","redMaPPer","clusters"],"reference_answer":"A total of 24,406 redMaPPer clusters were used after excluding clusters with redshift z < 0.2.","category":"numerical"}
{"question":"What completeness percentage does YOLO-CL achieve for redMaPPer clusters?","keywords":["95–98%","completeness","YOLO-CL"],"reference_answer":"YOLO-CL achieves a completeness between 95% and 98% for redMaPPer clusters.","category":"numerical"}
{"question":"What purity percentage does YOLO-CL achieve on SDSS blank fields?","keywords":["95–98%","purity","blank fields"],"reference_answer":"YOLO-CL achieves a purity of 95–98% when applied to SDSS blank fields.","category":"numerical"}
{"question":"At what X-ray luminosity does YOLO-CL recover all MCXC2021 clusters?","keywords":["2–3 × 10^44 erg/s","LX","MCXC2021"],"reference_answer":"YOLO-CL recovers all MCXC2021 clusters at X-ray luminosities LX ≳ 2–3 × 10^44 erg/s.","category":"numerical"}
{"question":"What redshift range shows approximately constant selection function for YOLO-CL?","keywords":["0.2 ≲ z ≲ 0.6","selection function","redshift"],"reference_answer":"YOLO-CL shows an approximately constant selection function with redshift in the range 0.2 ≲ z ≲ 0.6.","category":"numerical"}
{"question":"When is the YOLO-CL network applied during the cluster detection process?","keywords":["inference","single network pass","image-level detection"],"reference_answer":"YOLO-CL is applied during inference as a single network pass over the full image to simultaneously detect and localize galaxy clusters.","category":"temporal"}
{"question":"During which phase are SDSS images resized to 512×512 or 1024×1024 pixels?","keywords":["preprocessing","image resizing","training"],"reference_answer":"The SDSS images are resized during the preprocessing stage before training and validation of the YOLO-CL network.","category":"temporal"}
{"question":"When is the generalized Intersection over Union (gIoU) loss used in YOLO-CL?","keywords":["gIoU","training","loss function"],"reference_answer":"The generalized Intersection over Union loss is used during the training phase to optimize bounding box predictions.","category":"temporal"}
{"question":"How does YOLO-CL differ from traditional optical cluster detection algorithms?","keywords":["deep learning","no photometric catalogs","traditional methods"],"reference_answer":"YOLO-CL differs by using deep convolutional neural networks to detect clusters directly from images, while traditional methods rely on galaxy photometry, photometric redshifts, and spatial overdensity searches.","category":"comparative"}
{"question":"How does YOLO-CL compare to redMaPPer in detecting MCXC2021 clusters?","keywords":["higher completeness","MCXC2021","redMaPPer"],"reference_answer":"YOLO-CL is more complete than redMaPPer in detecting MCXC2021 clusters, particularly at lower X-ray surface brightness thresholds.","category":"comparative"}
{"question":"How does YOLO’s single-network approach differ from R-CNN based methods?","keywords":["single network","global context","R-CNN"],"reference_answer":"YOLO uses a single neural network that considers global image context, whereas R-CNN based methods require multiple region proposals and repeated network evaluations.","category":"comparative"}
{"question":"What limitations of image-level X-ray and SZ detection motivate optical deep learning approaches?","keywords":["faint signal","high redshift","X-ray","SZ"],"reference_answer":"At high redshift, clusters are less massive and produce fainter X-ray and SZ signals, reducing the effectiveness of traditional detection methods and motivating optical deep learning approaches like YOLO-CL.","category":"holistic"}
{"question":"What advantages does YOLO-CL provide for future large-scale cosmological surveys?","keywords":["LSST","Euclid","Roman","efficiency"],"reference_answer":"YOLO-CL provides fast, efficient, and highly complete cluster detection without relying on photometric catalogs, making it suitable for future surveys such as LSST, Euclid, and the Roman Space Telescope.","category":"holistic"}
{"question":"Why does YOLO-CL reduce systematic uncertainties compared to traditional methods?","keywords":["no photometric redshifts","systematic uncertainties"],"reference_answer":"YOLO-CL reduces systematic uncertainties because it does not depend on galaxy photometry or photometric redshift measurements, which can introduce errors during source detection and catalog construction.","category":"holistic"}
{"question":"What is the relationship between the encoder grid cells and bounding box predictions in YOLO-CL?","keywords":["S×S grid","bounding boxes","objectness"],"reference_answer":"The image is divided into an S×S grid, and for each grid cell YOLO-CL predicts bounding boxes with associated objectness probabilities representing the likelihood of a galaxy cluster.","category":"relationship"}
{"question":"What is the relationship between cluster richness λ and detection completeness?","keywords":["richness","λ","completeness"],"reference_answer":"YOLO-CL reaches approximately 98% completeness for clusters with richness λ ≳ 40, while completeness decreases to about 92–94% at lower richness values.","category":"relationship"}
{"question":"Describe the full training workflow used to construct the YOLO-CL cluster catalog.","keywords":["training","validation","blank fields","redMaPPer"],"reference_answer":"The workflow consists of training YOLO-CL on redMaPPer cluster images and an equal number of SDSS blank field images, validating on a held-out subset, optimizing loss using gIoU, and applying detection thresholds to build the final cluster catalog.","category":"spanning"}
{"question":"Describe the steps used to generate input images for YOLO-CL training.","keywords":["SDSS DR16","JPEG","g r i bands","ImgCutout"],"reference_answer":"The steps are: retrieve SDSS DR16 raw images, extract g, r, and i-band FITS files, generate color images using the Lupton et al. conversion algorithm via the ImgCutout service, and resize images to the network input resolution.","category":"spanning"}
{"question":"How are completeness and purity evaluated for YOLO-CL across redshift and richness?","keywords":["completeness","purity","redshift","richness"],"reference_answer":"Completeness and purity are evaluated by comparing YOLO-CL detections with the redMaPPer catalog as functions of redshift and richness, and by applying the network to SDSS blank fields to estimate false detections.","category":"spanning"}
{"question":"How does YOLO-CL integrate multi-scale feature maps to detect galaxy clusters of different sizes?","keywords":["multi-scale","Darknet-53","feature maps"],"reference_answer":"YOLO-CL uses multi-scale feature maps produced by Darknet-53 at different resolutions, allowing the detection network to identify galaxy clusters across a range of angular sizes.","category":"spanning"}
{"question":"What backbone network is used in RefineContourNet for object contour detection?","keywords":["RefineContourNet","ResNet","backbone network"],"reference_answer":"RefineContourNet uses a ResNet backbone network to leverage high-level abstraction capability for object contour detection.","category":"direct_fact"}
{"question":"What type of CNN architecture is proposed for object contour detection in RefineContourNet?","keywords":["multi-path refinement","CNN","object contour detection"],"reference_answer":"RefineContourNet proposes a ResNet-based multi-path refinement CNN for object contour detection.","category":"direct_fact"}
{"question":"What feature fusion order is used in RefineContourNet?","keywords":["high-level","mid-level","low-level","feature fusion"],"reference_answer":"RefineContourNet fuses high-level features first, followed by mid-level features, and finally low-level features.","category":"direct_fact"}
{"question":"What dataset is used to train RefineContourNet for object contour detection?","keywords":["PASCAL VOC 2012","object contour detection","training dataset"],"reference_answer":"RefineContourNet is trained on a modified PASCAL VOC 2012 dataset for object contour detection.","category":"direct_fact"}
{"question":"What loss function is used for contour detection in RefineContourNet?","keywords":["logistic regression loss","binary classification","contour detection"],"reference_answer":"RefineContourNet uses a logistic regression loss function for binary contour detection.","category":"direct_fact"}
{"question":"What value is used for the weighting factor β in the loss function?","keywords":["beta","β","weighting factor"],"reference_answer":"The weighting factor β is set to 10 to handle the imbalance between contour and non-contour pixels.","category":"numerical"}
{"question":"How many feature maps are used in the final layer for contour prediction?","keywords":["single feature map","binary classification","final layer"],"reference_answer":"A single feature map is used in the final layer for binary contour classification.","category":"numerical"}
{"question":"What Optimal Dataset Scale (ODS) score does RefineContourNet achieve on refined PASCAL val2012?","keywords":["ODS","0.752","PASCAL val2012"],"reference_answer":"RefineContourNet achieves an ODS score of 0.752 on the refined PASCAL val2012 dataset.","category":"numerical"}
{"question":"What ODS score is achieved by RefineContourNet on BSDS500 for edge detection?","keywords":["ODS","0.824","BSDS500"],"reference_answer":"RefineContourNet achieves an ODS score of 0.824 on the BSDS500 dataset for edge detection.","category":"numerical"}
{"question":"How many images are randomly selected per epoch during training?","keywords":["1000 images","epoch","training"],"reference_answer":"For each epoch, 1000 random images are selected from the training set.","category":"numerical"}
{"question":"When are high-level feature maps used in the refinement process?","keywords":["high-level features","starting point","refinement"],"reference_answer":"High-level feature maps are used at the beginning of the refinement process and serve as the starting point.","category":"temporal"}
{"question":"When is fine-training performed for edge detection on BSDS500?","keywords":["fine-training","edge detection","BSDS500"],"reference_answer":"Fine-training for edge detection is performed after training on object contour datasets, using BSDS500.","category":"temporal"}
{"question":"How does RefineContourNet differ from skip-layer architectures like HED?","keywords":["step-by-step refinement","skip-layer","HED"],"reference_answer":"RefineContourNet uses step-by-step refinement from deep to shallow features instead of simple skip-layer feature concatenation used in HED.","category":"comparative"}
{"question":"How does the backbone choice in RefineContourNet compare to earlier methods?","keywords":["ResNet","VGG","classification ability"],"reference_answer":"RefineContourNet uses a ResNet backbone with higher classification ability, whereas many earlier methods rely on VGG backbones.","category":"comparative"}
{"question":"How does RefineContourNet performance compare to CEDN and HED on PASCAL val2012?","keywords":["RCN","CEDN","HED","performance"],"reference_answer":"RefineContourNet outperforms CEDN and HED on PASCAL val2012 across ODS, OIS, and AP metrics.","category":"comparative"}
{"question":"Why is high-level abstraction capability important for object contour detection?","keywords":["high-level abstraction","semantic context","object contours"],"reference_answer":"High-level abstraction capability is important because it enables distinguishing meaningful object contours from background edges using semantic context.","category":"holistic"}
{"question":"What advantages does multi-path refinement provide in RefineContourNet?","keywords":["multi-path refinement","context integration","feature fusion"],"reference_answer":"Multi-path refinement allows effective integration of contextual information across different abstraction levels, improving contour localization and robustness.","category":"holistic"}
{"question":"How does the training strategy address limited object contour data?","keywords":["data augmentation","pre-training","fine-training"],"reference_answer":"The training strategy uses data augmentation, pre-training on a modified COCO dataset, and fine-training to compensate for limited object contour data.","category":"holistic"}
{"question":"What is the relationship between the encoder output and contour prediction?","keywords":["feature maps","binary prediction","sigmoid"],"reference_answer":"The encoder produces feature maps that are refined and passed through a sigmoid activation to predict contour presence probabilities.","category":"relationship"}
{"question":"What is the relationship between contour detection and edge detection in RefineContourNet?","keywords":["object contours","edge detection","subset"],"reference_answer":"Object contours are treated as a meaningful subset of edges, and the same architecture is adapted for both tasks.","category":"relationship"}
{"question":"Describe the sequence of refinement blocks used in RefineContourNet.","keywords":["RCU","MRF","CRP","refinement sequence"],"reference_answer":"The refinement sequence consists of Residual Convolution Units (RCU), followed by Multi-Resolution Fusion (MRF), and then Chained Residual Pooling (CRP).","category":"spanning"}
{"question":"Explain the data flow from deep features to pixel-wise contour prediction.","keywords":["deep features","refinement","pixel-wise prediction"],"reference_answer":"Deep feature maps from the ResNet backbone are refined layer-by-layer through multi-path refinement blocks and finally converted into pixel-wise contour probabilities.","category":"spanning"}
{"question":"How is class imbalance handled during contour detection training?","keywords":["class imbalance","beta weighting","loss function"],"reference_answer":"Class imbalance is handled by using a weighted logistic loss with β=10 to emphasize contour pixels over non-contour pixels.","category":"spanning"}
{"question":"How does RefineContourNet integrate original image information in the final refinement step?","keywords":["original image","extra refinement path","RCU"],"reference_answer":"An extra refinement path using the original image and three RCUs is added in the final step to improve contour detection results.","category":"spanning"}
{"question":"Which type of edges does RefineContourNet detect more effectively compared to other methods?","keywords":["inner contours","relevant edges","suppression"],"reference_answer":"RefineContourNet detects relevant edges such as inner object contours more effectively while suppressing undesired background edges.","category":"spanning"}
{"question":"What is the main goal of the survey on MLLMs described in the paper?","keywords":["survey","MLLMs","basic idea","main method","current progress"],"reference_answer":"The main goal of the survey is to provide researchers with a grasp of the basic idea, main methods, and current progress of multimodal large language models, with a focus on visual and language modalities.","category":"direct_fact"}
{"question":"Which modalities are mainly focused on in the survey of MLLMs?","keywords":["visual","language","modalities","MLLMs"],"reference_answer":"The survey mainly focuses on visual and language modalities, while also including works involving other modalities such as video and audio.","category":"direct_fact"}
{"question":"What are the three essential modules of a typical MLLM architecture?","keywords":["modality encoder","LLM","modality interface"],"reference_answer":"A typical MLLM consists of three essential modules: a pre-trained modality encoder, a pre-trained LLM, and a modality interface that connects them.","category":"direct_fact"}
{"question":"What role does the modality interface play in an MLLM?","keywords":["modality interface","alignment","modalities"],"reference_answer":"The modality interface aligns different modalities by projecting encoder outputs into a space that the LLM can understand efficiently.","category":"direct_fact"}
{"question":"Which pre-trained image-text model is commonly used as a modality encoder due to semantic alignment?","keywords":["CLIP","visual encoder","image-text"],"reference_answer":"CLIP is commonly used as a modality encoder because its visual encoder is semantically aligned with text through large-scale image-text pre-training.","category":"direct_fact"}
{"question":"How many modules are abstracted in a typical MLLM architecture?","keywords":["three modules","MLLM architecture"],"reference_answer":"A typical MLLM architecture is abstracted into three modules.","category":"numerical"}
{"question":"What is the parameter size of the OpenCLIP-ViT-bigG/14 image encoder?","keywords":["OpenCLIP-ViT-bigG/14","parameter size"],"reference_answer":"The OpenCLIP-ViT-bigG/14 image encoder has a parameter size of 1844.9 million.","category":"numerical"}
{"question":"How many image-caption pairs are included in the CC-12M dataset?","keywords":["CC-12M","12.4M","image-caption pairs"],"reference_answer":"The CC-12M dataset contains 12.4 million image-caption pairs.","category":"numerical"}
{"question":"How many samples are reported for the LAION-5B dataset?","keywords":["LAION-5B","5.85B","samples"],"reference_answer":"The LAION-5B dataset contains approximately 5.85 billion image-text pairs.","category":"numerical"}
{"question":"What is the sample size of the LLaVA-Instruct dataset generated by self-instruction?","keywords":["LLaVA-Instruct","158K","self-instruction"],"reference_answer":"The LLaVA-Instruct dataset contains 158K samples generated through self-instruction.","category":"numerical"}
{"question":"At which stage does pre-training occur in the full training pipeline of an MLLM?","keywords":["pre-training","first stage","training pipeline"],"reference_answer":"Pre-training occurs as the first stage in the full training pipeline of an MLLM.","category":"temporal"}
{"question":"When is instruction-tuning applied during MLLM training?","keywords":["instruction-tuning","training stage","after pre-training"],"reference_answer":"Instruction-tuning is applied after pre-training and before alignment tuning during MLLM training.","category":"temporal"}
{"question":"When is alignment tuning typically used in MLLMs?","keywords":["alignment tuning","human preferences","hallucinations"],"reference_answer":"Alignment tuning is typically used after instruction-tuning to align models with specific human preferences, such as reducing hallucinations.","category":"temporal"}
{"question":"How does token-level fusion differ from feature-level fusion in modality interfaces?","keywords":["token-level fusion","feature-level fusion","connector"],"reference_answer":"Token-level fusion transforms encoder features into tokens concatenated with text tokens before entering the LLM, whereas feature-level fusion inserts modules inside the LLM to enable deep interaction between visual and language features.","category":"comparative"}
{"question":"How does instruction tuning differ from supervised fine-tuning and prompting?","keywords":["instruction tuning","supervised fine-tuning","prompting"],"reference_answer":"Instruction tuning teaches models to generalize to unseen tasks via natural language instructions, unlike supervised fine-tuning which fits task-specific models and prompting which relies on prompt engineering with limited zero-shot generalization.","category":"comparative"}
{"question":"How does using higher input resolution compare to increasing parameter size for encoder selection?","keywords":["input resolution","parameter size","performance gains"],"reference_answer":"Empirical studies show that using higher input resolution yields more significant performance gains than increasing parameter size or changing training data composition.","category":"comparative"}
{"question":"What limitations of expert-model-based modality translation are discussed in the paper?","keywords":["expert models","information loss","spatial-temporal"],"reference_answer":"Expert-model-based translation may cause information loss, such as distorted spatial-temporal relationships when converting videos into textual descriptions.","category":"holistic"}
{"question":"What advantages are associated with higher-quality instruction-tuning data according to recent studies?","keywords":["data quality","instruction-tuning","performance"],"reference_answer":"Higher-quality instruction-tuning data can achieve better model performance than larger quantities of noisy data, improving generalization and reducing hallucinations.","category":"holistic"}
{"question":"What key challenges motivate the need for new evaluation benchmarks for MLLMs?","keywords":["evaluation","emergent capabilities","benchmarks"],"reference_answer":"The versatility of MLLMs and their emergent capabilities, such as OCR-free reasoning, motivate the need for comprehensive and specialized evaluation benchmarks.","category":"holistic"}
{"question":"What is the relationship between modality encoders and human sensory organs as described in the paper?","keywords":["modality encoder","human eyes","human ears"],"reference_answer":"Modality encoders are analogous to human eyes and ears, as they receive and preprocess raw optical or acoustic signals before reasoning.","category":"relationship"}
{"question":"What is the relationship between the Q-Former and token-level fusion in MLLMs?","keywords":["Q-Former","token-level fusion","learnable queries"],"reference_answer":"The Q-Former implements token-level fusion by using learnable query tokens to extract and compress visual features into tokens that can be processed by the LLM.","category":"relationship"}
{"question":"What is the relationship between instruction diversity and model generalization?","keywords":["instruction diversity","generalization","performance"],"reference_answer":"Greater instruction diversity improves model generalization ability and overall performance, as empirically verified by prior studies.","category":"relationship"}
{"question":"Describe the full training pipeline of an MLLM from pre-training to alignment tuning.","keywords":["pre-training","instruction-tuning","alignment tuning"],"reference_answer":"The training pipeline consists of pre-training to align modalities and learn world knowledge, instruction-tuning to enable task following via natural language instructions, and alignment tuning to align model outputs with human preferences using methods like RLHF or DPO.","category":"spanning"}
{"question":"Explain how coarse-grained and fine-grained pretraining data differ in purpose and characteristics.","keywords":["coarse-grained","fine-grained","pretraining data"],"reference_answer":"Coarse-grained data are large-scale, web-scraped, short and noisy captions used mainly for modality alignment, while fine-grained data contain longer and more accurate descriptions that enable finer alignment between image and text.","category":"spanning"}
{"question":"How are closed-set and open-set evaluations distinguished in MLLM assessment?","keywords":["closed-set","open-set","evaluation"],"reference_answer":"Closed-set evaluation uses predefined answer options and benchmark metrics on specific datasets, while open-set evaluation allows free-form responses and relies on manual scoring, GPT scoring, or case studies.","category":"spanning"}
{"question":"What is the main goal of EVOR in retrieval-augmented code generation?","keywords":["EVOR","retrieval-augmented","code generation","external knowledge"],"reference_answer":"EVOR aims to enhance retrieval-augmented code generation by enabling LLMs to effectively utilize external knowledge such as documentation, execution feedback, and code snippets without updating model parameters.","category":"direct_fact"}
{"question":"What types of external knowledge are included in the EVOR knowledge base K by default?","keywords":["documentation","execution feedback","code snippets","knowledge base"],"reference_answer":"By default, the EVOR knowledge base includes documentation, execution feedback, and code snippets.","category":"direct_fact"}
{"question":"What role does execution feedback play in EVOR?","keywords":["execution feedback","syntax errors","debugging","knowledge"],"reference_answer":"Execution feedback exposes syntax mistakes and code errors by executing generated programs with a compiler or interpreter, and is used to evolve both queries and the knowledge base.","category":"direct_fact"}
{"question":"What is the purpose of code snippets in EVOR?","keywords":["code snippets","sample usage","syntax","grammar"],"reference_answer":"Code snippets provide concrete examples of function usage, syntax, inputs, outputs, and grammar, and serve as demonstrations for guiding code generation.","category":"direct_fact"}
{"question":"What is the input and output of the EVOR pipeline as defined in Algorithm 1?","keywords":["Algorithm 1","input","output","EVOR pipeline"],"reference_answer":"The input is a coding problem description n, and the output is the generated code p.","category":"direct_fact"}
{"question":"What is the maximum number of iterations used in EVOR experiments?","keywords":["30","maximum iterations","EVOR"],"reference_answer":"The maximum number of iterations used in EVOR experiments is 30.","category":"numerical"}
{"question":"How many consecutive identical execution feedbacks trigger termination in EVOR?","keywords":["3","termination condition","execution feedback"],"reference_answer":"The EVOR pipeline terminates if the same execution feedback occurs in consecutive 3 iterations.","category":"numerical"}
{"question":"What is the maximum context length allowed for ChatGPT and CodeLlama in EVOR?","keywords":["4096","context length","ChatGPT","CodeLlama"],"reference_answer":"The maximum context length allowed for both ChatGPT and CodeLlama is 4,096 tokens.","category":"numerical"}
{"question":"How many datasets are introduced to evaluate EVOR generalization capability?","keywords":["four datasets","updated libraries","long-tail languages"],"reference_answer":"Four datasets are introduced to evaluate EVOR, including two updated libraries and two long-tail programming languages.","category":"numerical"}
{"question":"Which metric is used as the default evaluation metric throughout the paper?","keywords":["pass@1","execution accuracy","metric"],"reference_answer":"Execution accuracy measured by pass@1 is used as the default evaluation metric.","category":"direct_fact"}
{"question":"When is execution feedback Fi obtained in the EVOR pipeline?","keywords":["execution","generated program","test inputs"],"reference_answer":"Execution feedback Fi is obtained after generating the program pi and executing it on LLM-generated test inputs using a compiler or interpreter.","category":"temporal"}
{"question":"When does the EVOR pipeline update the knowledge base with a code snippet?","keywords":["successful execution","knowledge base update","EVOR"],"reference_answer":"The knowledge base is updated with the generated code snippet when the execution is successful.","category":"temporal"}
{"question":"At what stage are test inputs generated in Algorithm 1?","keywords":["test inputs","initial iteration","Mt"],"reference_answer":"Test inputs are generated after the first iteration, when i equals 0, using the model Mt.","category":"temporal"}
{"question":"How does EVOR differ from DocPrompting in knowledge utilization?","keywords":["EVOR","DocPrompting","knowledge evolution","retrieval"],"reference_answer":"EVOR evolves both queries and the knowledge base across iterations, while DocPrompting retrieves documentation as a single static source without evolution.","category":"comparative"}
{"question":"How does synchronous evolution compare to evolving only queries or only knowledge?","keywords":["synchronous evolution","query evolution","knowledge evolution"],"reference_answer":"Synchronous evolution of both queries and knowledge consistently outperforms evolving only queries or only knowledge across all datasets.","category":"comparative"}
{"question":"How does EVOR performance compare to vanilla generation on EVOR-BENCH?","keywords":["EVOR","vanilla generation","performance"],"reference_answer":"EVOR significantly outperforms vanilla generation, achieving much higher execution accuracy across all datasets.","category":"comparative"}
{"question":"Why is general web search considered less effective as a knowledge source in EVOR?","keywords":["web search","noisy information","performance"],"reference_answer":"General web search contains noisy information and provides only marginal improvements, making it less effective than code snippets or documentation.","category":"holistic"}
{"question":"What advantages does diverse knowledge soup provide in EVOR?","keywords":["diverse knowledge","knowledge soup","RACG"],"reference_answer":"A diverse knowledge soup improves RACG performance by integrating complementary information from documentation, code snippets, and execution feedback, especially when combined with evolution.","category":"holistic"}
{"question":"Why is EVOR suitable for updated libraries and long-tail programming languages?","keywords":["updated libraries","long-tail languages","external knowledge"],"reference_answer":"EVOR is suitable because it explicitly retrieves and evolves external knowledge that LLMs have not seen during training, enabling better generalization.","category":"holistic"}
{"question":"What is the relationship between execution feedback and knowledge evolution in EVOR?","keywords":["execution feedback","knowledge evolution","error messages"],"reference_answer":"Execution feedback contributes to knowledge evolution by adding either syntax-correct code snippets or code-error message pairs to the knowledge base.","category":"relationship"}
{"question":"What is the relationship between query evolution and retrieval quality in EVOR?","keywords":["query evolution","retrieval","relevant knowledge"],"reference_answer":"Query evolution improves retrieval quality by refining queries based on previous programs and execution feedback to retrieve more relevant knowledge.","category":"relationship"}
{"question":"What is the relationship between EVOR and agent-based methods like SWE-agent?","keywords":["EVOR","SWE-agent","integration"],"reference_answer":"EVOR can be integrated with agent-based methods like SWE-agent by augmenting the search space with execution feedback and code snippets, leading to further performance improvements.","category":"relationship"}
{"question":"Describe the step-by-step process of one EVOR iteration.","keywords":["query formulation","retrieval","generation","execution","knowledge evolution"],"reference_answer":"In one iteration, EVOR formulates a query qi, retrieves relevant knowledge Kr, generates a program pi, executes it to obtain feedback Fi, and updates the knowledge base with either pi or the pair (pi, Fi).","category":"spanning"}
{"question":"Explain how synchronous evolution improves RACG performance across datasets.","keywords":["synchronous evolution","queries","knowledge","performance"],"reference_answer":"By evolving both queries and knowledge simultaneously, EVOR leverages their complementary strengths, leading to consistently higher execution accuracy across all datasets.","category":"spanning"}
{"question":"How does EVOR achieve more effective token usage compared to DocPrompting?","keywords":["token budget","effective token usage","EVOR"],"reference_answer":"EVOR achieves more effective token usage by iteratively evolving queries and knowledge, resulting in higher pass@t scores at all token consumption levels compared to DocPrompting.","category":"spanning"}
{"question":"What problem does fixed-size chunking introduce in classic RAG systems?","keywords":["fixed-size chunking","fragmented context","incomplete retrieval","coherence"],"reference_answer":"Fixed-size chunking fragments document context, which can lead to incomplete retrieval and reduced coherence in generated responses.","category":"direct_fact"}
{"question":"What two advanced techniques are analyzed to address context loss in RAG?","keywords":["late chunking","contextual retrieval","RAG"],"reference_answer":"The two techniques analyzed are late chunking and contextual retrieval.","category":"direct_fact"}
{"question":"What is the main goal of contextual retrieval in RAG systems?","keywords":["contextual retrieval","semantic coherence","global context"],"reference_answer":"The main goal of contextual retrieval is to preserve semantic coherence by enriching each chunk with global document context.","category":"direct_fact"}
{"question":"What is the key idea behind late chunking?","keywords":["late chunking","token-level embedding","global context"],"reference_answer":"Late chunking embeds the entire document at the token level first and performs chunking afterward to preserve global context.","category":"direct_fact"}
{"question":"What are the two core components of a RAG model?","keywords":["retrieval mechanism","generative model","RAG"],"reference_answer":"A RAG model consists of a retrieval mechanism and a generative model.","category":"direct_fact"}
{"question":"What chunk size is used for fixed-size segmentation in the experiments?","keywords":["512 characters","fixed-size segmentation","chunk size"],"reference_answer":"Fixed-size segmentation uses chunks of 512 characters.","category":"numerical"}
{"question":"What percentage of the NFCorpus dataset was used for RQ#2 experiments?","keywords":["20%","NFCorpus","RQ#2"],"reference_answer":"Only 20% of the NFCorpus dataset was used for RQ#2 experiments.","category":"numerical"}
{"question":"How many queries were evaluated in RQ#1?","keywords":["1,000 queries","RQ#1","evaluation"],"reference_answer":"RQ#1 was evaluated using the first 1,000 queries.","category":"numerical"}
{"question":"How many queries were evaluated in RQ#2?","keywords":["50 queries","RQ#2","evaluation"],"reference_answer":"RQ#2 was evaluated using 50 queries.","category":"numerical"}
{"question":"What GPU and VRAM configuration was used in the experiments?","keywords":["RTX 4090","24GB VRAM","hardware"],"reference_answer":"The experiments used an Nvidia RTX 4090 GPU with 24GB of VRAM.","category":"numerical"}
{"question":"When does chunking occur in the late chunking workflow?","keywords":["after embedding","late chunking","workflow"],"reference_answer":"In late chunking, chunking occurs after the entire document has been embedded at the token level.","category":"temporal"}
{"question":"When is contextual information added to chunks in contextual retrieval?","keywords":["before embedding","contextualization","chunks"],"reference_answer":"Contextual information is added to each chunk before embedding.","category":"temporal"}
{"question":"At which stage is reranking applied in contextual retrieval with rank fusion?","keywords":["after retrieval","reranking","two-stage"],"reference_answer":"Reranking is applied after the initial retrieval stage.","category":"temporal"}
{"question":"How does contextual retrieval differ from traditional early chunking?","keywords":["contextual retrieval","early chunking","global context"],"reference_answer":"Contextual retrieval enriches each chunk with document-level context, while early chunking embeds isolated chunks without global context.","category":"comparative"}
{"question":"How does late chunking compare to early chunking in preserving context?","keywords":["late chunking","early chunking","context preservation"],"reference_answer":"Late chunking preserves global context by embedding the full document first, whereas early chunking processes chunks independently.","category":"comparative"}
{"question":"How does dense embedding retrieval differ from BM25 sparse retrieval?","keywords":["dense embeddings","BM25","semantic","lexical"],"reference_answer":"Dense embeddings capture semantic similarity, while BM25 focuses on exact lexical matches using TF-IDF-based scoring.","category":"comparative"}
{"question":"Why does contextual retrieval require more computational resources?","keywords":["LLM prompting","VRAM usage","context generation"],"reference_answer":"Contextual retrieval requires prompting an LLM to generate context for each chunk, increasing VRAM usage and computation time.","category":"holistic"}
{"question":"What trade-off between efficiency and relevance is observed between late chunking and contextual retrieval?","keywords":["efficiency","relevance","late chunking","contextual retrieval"],"reference_answer":"Late chunking is more efficient but may reduce relevance and completeness, while contextual retrieval improves semantic coherence at higher computational cost.","category":"holistic"}
{"question":"Why is chunking necessary in RAG despite long-context LLMs?","keywords":["context window","positional bias","chunking"],"reference_answer":"Chunking is necessary because most LLMs have limited context windows and exhibit positional bias, making it difficult to process long documents effectively.","category":"holistic"}
{"question":"What is the relationship between contextualization and retrieval relevance?","keywords":["contextualization","retrieval relevance","semantic integrity"],"reference_answer":"Contextualization improves retrieval relevance by ensuring each chunk retains semantic integrity from the original document.","category":"relationship"}
{"question":"What is the relationship between rank fusion and retrieval accuracy?","keywords":["rank fusion","dense embeddings","BM25"],"reference_answer":"Rank fusion improves retrieval accuracy by combining dense semantic embeddings with BM25 sparse lexical matching.","category":"relationship"}
{"question":"What is the relationship between VRAM limitations and dataset size in RQ#2?","keywords":["VRAM constraints","dataset subset","RQ#2"],"reference_answer":"Due to VRAM constraints, only a subset of the NFCorpus dataset could be used in RQ#2 experiments.","category":"relationship"}
{"question":"Describe the end-to-end process of contextual retrieval with rank fusion.","keywords":["contextualization","dense retrieval","BM25","rank fusion","reranking"],"reference_answer":"The process includes chunking documents, generating context for each chunk, embedding chunk-plus-context, retrieving using dense and BM25 embeddings, applying weighted rank fusion, and reranking the retrieved chunks.","category":"spanning"}
{"question":"Describe the workflow differences evaluated in RQ#1 and RQ#2.","keywords":["RQ#1","RQ#2","late chunking","contextual retrieval"],"reference_answer":"RQ#1 evaluates early versus late chunking strategies using different embedding models, while RQ#2 compares contextual retrieval with traditional early chunking using contextualization, rank fusion, and reranking.","category":"spanning"}
{"question":"How do embedding models and segmentation strategies interact in the experimental setup?","keywords":["embedding models","segmentation","retrieval quality"],"reference_answer":"Different segmentation strategies are paired with embedding models to assess how chunk boundaries and embeddings jointly affect retrieval quality and downstream generation performance.","category":"spanning"}
{"question":"What problem does TOR aim to address in multi-hop question answering?","keywords":["multi-hop question answering","cascading errors","iterative retrieval","TOR"],"reference_answer":"TOR aims to address cascading errors and misleading retrievals that occur in chain-like iterative retrieval methods for multi-hop question answering.","category":"direct_fact"}
{"question":"What is the core idea of the TREE OF REVIEWS framework?","keywords":["TREE OF REVIEWS","tree structure","dynamic retrieval","reasoning paths"],"reference_answer":"The core idea of TREE OF REVIEWS is to use a tree structure where the question is the root and each retrieved paragraph is treated as an independent node, enabling multiple reasoning paths.","category":"direct_fact"}
{"question":"What serves as the root node in the TOR framework?","keywords":["root node","question Q","TOR"],"reference_answer":"The root node in the TOR framework is the original question Q.","category":"direct_fact"}
{"question":"What does each non-root node represent in the TOR tree?","keywords":["paragraph","node","retrieval"],"reference_answer":"Each non-root node represents a single retrieved paragraph.","category":"direct_fact"}
{"question":"What actions can the paragraphs review block select in TOR?","keywords":["Accept","Reject","Search","paragraphs review"],"reference_answer":"The paragraphs review block can select one of three actions: Accept, Reject, or Search.","category":"direct_fact"}
{"question":"How many multi-hop question answering datasets are used to evaluate TOR?","keywords":["three datasets","evaluation","multi-hop"],"reference_answer":"Three multi-hop question answering datasets are used to evaluate TOR.","category":"numerical"}
{"question":"What is the maximum number of paragraphs allowed when computing recall@15?","keywords":["15","recall@15","paragraphs"],"reference_answer":"The maximum number of paragraphs allowed when computing recall@15 is 15.","category":"numerical"}
{"question":"How many questions were used from each development set for main evaluation?","keywords":["500 questions","development set","evaluation"],"reference_answer":"The first 500 questions from each development set were used for main evaluation.","category":"numerical"}
{"question":"What tree depth and widths are finally chosen for TOR in experiments?","keywords":["depth 3","5,3,3","tree structure"],"reference_answer":"The chosen tree depth is 3, with widths of 5, 3, and 3.","category":"numerical"}
{"question":"Which retrieval metric is used to evaluate retrieval quality in TOR?","keywords":["recall@15","retrieval metric"],"reference_answer":"Retrieval quality is evaluated using recall@15.","category":"direct_fact"}
{"question":"When does TOR stop expanding a reasoning path?","keywords":["Accept","Reject","maximum depth","termination"],"reference_answer":"TOR stops expanding a reasoning path when it is accepted, rejected, or reaches the maximum search depth.","category":"temporal"}
{"question":"When is a new query generated in the TOR framework?","keywords":["Search action","new query","paragraphs review"],"reference_answer":"A new query is generated when the paragraphs are relevant but do not contain enough information and the action selected is Search.","category":"temporal"}
{"question":"When is a piece of evidence added to the evidence pool?","keywords":["Accept","evidence pool","brief analysis"],"reference_answer":"A piece of evidence is added to the evidence pool when a reasoning path is accepted.","category":"temporal"}
{"question":"How does TOR differ from chain-like iterative retrieval methods such as IRCoT?","keywords":["tree structure","chain-like","IRCoT","comparison"],"reference_answer":"TOR uses a tree structure to explore multiple reasoning paths independently, while chain-like methods such as IRCoT follow a single sequential reasoning path.","category":"comparative"}
{"question":"How does TOR compare to one-time retrieval methods for multi-hop questions?","keywords":["one-time retrieval","iterative retrieval","multi-hop"],"reference_answer":"TOR outperforms one-time retrieval methods by iteratively retrieving and reviewing paragraphs to gather indirect facts required for multi-hop questions.","category":"comparative"}
{"question":"How does CoR differ from TOR in structure?","keywords":["CoR","single path","tree","chain"],"reference_answer":"CoR uses a single reasoning path with the same prompts as TOR, whereas TOR expands multiple reasoning paths using a tree structure.","category":"comparative"}
{"question":"Why does TOR reduce the impact of irrelevant paragraphs on reasoning?","keywords":["irrelevant paragraphs","tree structure","pruning"],"reference_answer":"TOR reduces the impact of irrelevant paragraphs by handling each paragraph as a separate node and applying pruning strategies to filter unproductive paths.","category":"holistic"}
{"question":"Why is iterative retrieval necessary for multi-hop question answering?","keywords":["iterative retrieval","indirect facts","multi-hop"],"reference_answer":"Iterative retrieval is necessary because multi-hop questions often require indirect facts that have little lexical overlap with the original question.","category":"holistic"}
{"question":"Why does the tree structure improve robustness compared to chain structures?","keywords":["multiple reasoning paths","error accumulation","tree"],"reference_answer":"The tree structure improves robustness by distributing risk across multiple reasoning paths, reducing the effect of a single reasoning error.","category":"holistic"}
{"question":"What is the relationship between retrieval quality and generation quality in TOR?","keywords":["retrieval metrics","generation metrics","correlation"],"reference_answer":"Retrieval quality is positively correlated with generation quality, meaning better retrieval leads to better final answers.","category":"relationship"}
{"question":"What is the relationship between pruning strategies and time overhead?","keywords":["pruning","time cost","API calls"],"reference_answer":"Pruning strategies reduce time overhead by decreasing redundant and irrelevant retrieval expansions, thereby reducing API calls.","category":"relationship"}
{"question":"What is the relationship between evidence fusion and reader performance?","keywords":["evidence fusion","reader","performance"],"reference_answer":"Evidence fusion improves reader performance by allowing the model to utilize information from multiple accepted reasoning paths.","category":"relationship"}
{"question":"Describe the step-by-step decision process in the paragraphs review block.","keywords":["relevance","enough information","Accept","Reject","Search"],"reference_answer":"The paragraphs review block first judges relevance to the question, then checks if the information is sufficient; it selects Reject if irrelevant, Search if relevant but insufficient, or Accept if relevant and sufficient.","category":"spanning"}
{"question":"Describe how pruning and effective expansion work together in TOR.","keywords":["pruning","effective expansion","search efficiency"],"reference_answer":"Pruning removes irrelevant or repetitive paths to reduce redundancy, while effective expansion guides query generation to retrieve more informative paragraphs, together improving efficiency and diversity.","category":"spanning"}
{"question":"Explain the overall retrieval-to-answer workflow in the TOR framework.","keywords":["retrieve-and-read","tree","evidence pool","final answer"],"reference_answer":"TOR retrieves paragraphs to build a tree, reviews each path to accept or reject evidence, stores accepted paths in an evidence pool, and generates the final answer based on fused evidence.","category":"spanning"}
{"question":"What is the main objective of the proposed convolution kernels in this paper?","keywords":["convolution kernels","electromagnetism","computer vision","shape analysis","stroke analysis"],"reference_answer":"The main objective is to present novel convolution kernels based on electromagnetic potentials and fields for general computer vision use and to demonstrate their application in shape and stroke analysis.","category":"direct_fact"}
{"question":"What physical principles are used to construct the proposed convolution kernels?","keywords":["electromagnetic potentials","electromagnetic fields","Maxwell"],"reference_answer":"The convolution kernels are constructed using principles of electromagnetic potentials and electromagnetic fields derived from Maxwell’s equations.","category":"direct_fact"}
{"question":"What is the name of the proposed method introduced in the paper?","keywords":["CAMERA-I","Convolution Approach of Magnetic and Electric Repulsion"],"reference_answer":"The proposed method is called CAMERA-I, which stands for Convolution Approach of Magnetic and Electric Repulsion to Analyze an Image.","category":"direct_fact"}
{"question":"Which image analysis tasks are emphasized as applications of electromagnetic kernels?","keywords":["shape analysis","stroke analysis","computer vision"],"reference_answer":"The paper emphasizes the application of electromagnetic kernels to shape analysis and stroke analysis in computer vision.","category":"direct_fact"}
{"question":"What common image processing technique do electromagnetic kernels belong to?","keywords":["image filtering","image convolution","feature extraction"],"reference_answer":"Electromagnetic kernels are used as image filtering operators based on image convolution for feature extraction.","category":"direct_fact"}
{"question":"What is the typical maximum size of conventional convolution kernels mentioned in related work?","keywords":["7×7","9×9","11×11","kernel size"],"reference_answer":"Conventional convolution kernels are usually small, with typical maximum sizes of 7×7, 9×9, or 11×11.","category":"numerical"}
{"question":"How many main objectives does the proposed approach CAMERA-I have?","keywords":["two objectives","CAMERA-I"],"reference_answer":"The proposed CAMERA-I approach has two main objectives.","category":"numerical"}
{"question":"What spatial dimension value corresponds to the real electric potential in the adapted equations?","keywords":["n=3","spatial dimensions","electric potential"],"reference_answer":"When the number of spatial dimensions n equals 3, the potential is identical to the real electric potential in three-dimensional space.","category":"numerical"}
{"question":"What condition is imposed on the kernel matrix size when creating a monopole potential kernel?","keywords":["odd number","kernel matrix","center pixel"],"reference_answer":"The kernel matrix must have an odd number of elements so that a central pixel represents the particle location.","category":"numerical"}
{"question":"At what stage are electromagnetic laws simplified for computer vision usage?","keywords":["static terms","simplification","computer vision"],"reference_answer":"Electromagnetic laws are simplified at the theoretical stage by keeping only static terms and removing physical constants to adapt them for computer vision.","category":"temporal"}
{"question":"When is the potential calculated relative to the vector field in the proposed method?","keywords":["potential","vector field","gradient"],"reference_answer":"The potential is calculated first, and the vector field is then obtained as the gradient of the potential.","category":"temporal"}
{"question":"When is the monopole potential kernel manually constructed in the convolution process?","keywords":["kernel construction","monopole potential","discrete grid"],"reference_answer":"The monopole potential kernel is manually constructed before convolution, using a discrete grid representing a single charged particle.","category":"temporal"}
{"question":"How does CAMERA-I differ from convolutional neural networks in kernel design?","keywords":["CAMERA-I","CNN","kernel optimization"],"reference_answer":"CAMERA-I uses analytically defined electromagnetic kernels based on physical laws, whereas CNNs rely on numerically optimized kernels learned from data.","category":"comparative"}
{"question":"How do electromagnetic kernels differ from gravity-based edge detection methods?","keywords":["electromagnetic fields","gravitational fields","edge detection"],"reference_answer":"Electromagnetic kernels are based on electromagnetic potentials and fields, while gravity-based methods rely on gravitational field analogies that are mathematically similar but less expressive.","category":"comparative"}
{"question":"How does the proposed approach compare to contour or skeleton-based shape representations?","keywords":["contour","skeleton","dimensionality reduction"],"reference_answer":"Unlike contour or skeleton-based methods that reduce shapes from 2D to 1D, the proposed approach preserves full dimensional information during analysis.","category":"comparative"}
{"question":"Why are electromagnetic kernels considered robust to noise and deformation?","keywords":["robustness","noise","deformation","EMPF"],"reference_answer":"Electromagnetic kernels are robust because the electromagnetic potential and field consider contributions from all pixels, making them less sensitive to local noise or deformations.","category":"holistic"}
{"question":"Why does the proposed approach allow long-distance stroke interaction analysis?","keywords":["long distance interaction","strokes","electromagnetic fields"],"reference_answer":"Long-distance stroke interaction is possible because electromagnetic fields propagate over the entire image and accumulate contributions from distant pixels.","category":"holistic"}
{"question":"What advantages do electromagnetic kernels provide over standard filtering methods?","keywords":["resolution independence","size independence","3D images","robustness"],"reference_answer":"Electromagnetic kernels provide resolution and size independence, robustness to noise and deformation, support for long-distance interactions, and the ability to analyze 3D images.","category":"holistic"}
{"question":"What is the relationship between the encoder equation and the potential computation?","keywords":["potential","gradient","vector field"],"reference_answer":"The vector field is directly related to the potential through the gradient operator, meaning the field is derived from spatial changes in the potential.","category":"relationship"}
{"question":"What is the relationship between monopoles and dipoles in the proposed framework?","keywords":["monopoles","dipoles","opposite charges"],"reference_answer":"Dipoles are formed by placing two monopoles of opposite signs close together, producing combined potentials and vector fields.","category":"relationship"}
{"question":"What is the relationship between potential values and the center of mass of a shape?","keywords":["potential","center of mass","concave regions"],"reference_answer":"The potential tends to be higher near the center of mass or in concave regions of a shape filled with monopoles.","category":"relationship"}
{"question":"Describe the steps used to compute electromagnetic fields from an image.","keywords":["convolution","potential","gradient","vector field"],"reference_answer":"First, the image is convolved with a monopole potential kernel to compute the scalar potential, and then the vector field is obtained by applying the gradient operator to the potential.","category":"spanning"}
{"question":"Describe how the adapted electromagnetic equations differ from physical Maxwell equations.","keywords":["static assumption","no constants","multi-dimensional"],"reference_answer":"The adapted equations ignore physical constants, assume static charges, allow magnetic monopoles, and generalize to arbitrary spatial dimensions for computer vision use.","category":"spanning"}
{"question":"Explain how shape features can be inferred from potential and field magnitudes.","keywords":["potential magnitude","field magnitude","concave","convex"],"reference_answer":"By jointly analyzing potential and field magnitudes, regions can be classified as concave, convex, near or far from the center of mass, enabling shape feature extraction.","category":"spanning"}
